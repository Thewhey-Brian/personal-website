---
title: "Building a ChatGPT from Scratch: A Dive into LLM Training"
summary: "A hands-on journey through the complete LLM pipelineâ€”tokenization, pretraining, distributed training, and fine-tuningâ€”inspired by Andrej Karpathy's nanochat. Training a 561M parameter conversational model on 11.2B tokens to understand different aspects of modern language models."
status: "completed"
role: "ML Engineering & Research"
stack: ["PyTorch", "CUDA", "Distributed Training", "BPE Tokenization", "Transformer Architecture"]
tags: ["llm", "deep learning", "nlp", "transformer", "distributed training", "self-supervised learning", "chatgpt"]
featured: true
startDate: "2025-10-16"
---

<div style={{width: "100%", height: "600px", border: "1px solid #e5e7eb", borderRadius: "8px", overflow: "hidden", marginBottom: "2rem"}}>
  <iframe
    src="https://brianguo-nanochat-20b-chat.hf.space"
    width="100%"
    height="100%"
    style={{border: "none"}}
    title="nanochat-20b-chat Demo"
  />
</div>

*Try the model above â€” it's a 561M parameter model trained from scratch. It makes mistakes, it hallucinates, and it's a bit naive, but it's fun.*

---

## Motivation

Large Language Models have transformed AI, but their training pipelines remain opaque to myself. When I encountered [Andrej Karpathy's nanochat project](https://github.com/karpathy/nanochat), I saw an opportunity to **get my hands dirty** with every stage of LLM developmentâ€”not just fine-tuning pre-trained models, but building one from the ground up.

This project wasn't about competing with GPT-4. It was about **understanding the fundamentals**:
- How does tokenization affect model efficiency?
- What happens during distributed pretraining?
- How do you transition from next-token prediction to conversational AI?
- What trade-offs exist between model size, data, and compute?

As someone aiming to apply deep learning to **model biological systems**, these foundational skillsâ€”handling large-scale data, distributed training, and representation learningâ€”are directly transferable. The techniques that enable LLMs to compress human language into vector spaces can similarly help compress the complexity of cellular processes, gene expression patterns, and protein interactions.

---

## Technical Overview

### Architecture: GPT-Style Decoder-Only Transformer

**Model Specifications**:
- **Parameters**: 561 million (d20 configuration: 20 layers)
- **Context Length**: 2048 tokens
- **Vocabulary**: 65,536 tokens (custom BPE tokenizer)
- **Hidden Dimension**: 1024
- **Attention Heads**: 16
- **Activation**: GELU

**Training Infrastructure**:
- **Hardware**: 8Ã— NVIDIA A100-SXM4-80GB GPUs (634GB total VRAM)
- **Framework**: PyTorch 2.8.0 with Distributed Data Parallel (DDP)
- **Platform**: Lambda Labs cloud (CUDA 12.8 on Linux)
- **Total Cost**: ~$120 (8.4 hours at $14.32/hour)
- **Model FLOPs Utilization (MFU)**: 20.82%

---

## The Complete Training Pipeline

### Stage 1: Custom Tokenizer Training

Instead of using GPT-2's tokenizer, a **custom Byte Pair Encoding (BPE) tokenizer** was trained from scratch on 2 billion characters from the FineWeb-Edu dataset.

**Performance vs. Baselines**:

| Domain | vs GPT-2 | vs GPT-4 |
|--------|----------|----------|
| News | **+7.2%** | **+3.1%** |
| Science | **+12.3%** | **+8.4%** |
| Code | **+14.4%** | -59.5% |
| Math | -3.2% | -16.1% |

**Key Insight**: The tokenizer excels at **scientific text and natural language** (the training domain) but underperforms on code and multilingual data (expected, since FineWeb-Edu is English-heavy). This demonstrates the importance of **domain-matched tokenization**, a critical consideration for future applications in computational biology where domain-specific vocabularies (gene names, protein sequences, chemical compounds) require specialized tokenization strategies.

**Compression Ratio**: 4.91 bytes/token (competitive with GPT-2's 4.67 and GPT-4's 4.81)

**Training Time**: 1.6 minutes on 8xA100

---

### Stage 2: Base Model Pretraining (21,400 iterations)

This is where the model learns **language understanding** through next-token prediction on unlabeled text.

**Training Details**:
- **Data**: FineWeb-Edu dataset (filtered Wikipedia and educational web content)
- **Tokens Processed**: 11.2 billion tokens (20:1 token-to-parameter ratio)
- **Batch Size**: 524,288 tokens (distributed across 8 GPUs)
- **Learning Rate Schedule**:
  - Matrix parameters: 0.02 (cosine decay with 20% warmdown)
  - Embedding: 0.2 (10Ã— higher for faster vocabulary learning)
  - Unembedding: 0.004 (lower to stabilize output logits)
- **Optimizer**: AdamW with weight decay 0.0
- **Training Time**: 6.6 hours
- **Final Validation Loss**: 0.8156 bits per byte

**Why Different Learning Rates?**
This tiered approach (borrowed from nanochat's modded-nanoGPT optimizations) stabilizes training:
- **High embedding LR**: Vocabulary embeddings need to move quickly to capture semantic relationships
- **Low unembedding LR**: Output layer overfitting prevention
- **Medium matrix LR**: Balances generalization and convergence

**Base Model Evaluation (CORE Benchmark)**:

| Task | Score | Interpretation |
|------|-------|----------------|
| HellaSwag | 0.2559 | Commonsense reasoning |
| Winograd | 0.3040 | Pronoun disambiguation |
| ARC-Easy | 0.5174 | Elementary science questions |
| ARC-Challenge | 0.1251 | Advanced reasoning |
| LAMBADA | 0.3775 | Long-range context |
| SQuAD | 0.2260 | Reading comprehension |

**CORE Score**: 0.2087 (composite metric showing GPT-1.5 level performance)

**Example Completions** (before chat tuning):
```
Prompt: "The capital of France is"
Output: "Paris. It is the largest city in France and the capital of the country."

Prompt: "The chemical symbol of gold is"
Output: "Au. It is a soft, silvery-white metal that is malleable and ductile."
```

---

### Stage 3: Midtraining (765 iterations)

**Purpose**: Domain adaptation to chat-style formatting and more diverse text sources.

**Data**: Transition from pure web text to conversational/instruction-following data

**Training Time**: 30 minutes

**Minimum Validation Loss**: 0.3976 bpb (significant drop from base model, indicating successful adaptation)

---

### Stage 4: Supervised Fine-Tuning (SFT) for Chat (651 iterations)

This stage transforms the base model into a conversational assistant by training on **20,843 human-AI conversations**.

**Training Configuration**:
- **Data**: SmolTalk dataset (HuggingFace)
- **Epochs**: 1 (to avoid overfitting on small instruction dataset)
- **Batch Size**: Effective 32 examples per step (4 per GPU Ã— 8 GPUs)
- **Loss Masking**: Only compute loss on AI responses (not user prompts)
- **Training Time**: 24 minutes
- **Final Validation Loss**: 1.0189

**Chat Model Benchmarks**:

| Benchmark | Score | Task Type |
|-----------|-------|-----------|
| ARC-Easy | 0.4571 | Science QA |
| ARC-Challenge | 0.3430 | Complex reasoning |
| MMLU | 0.3396 | Multitask knowledge |
| GSM8K | 0.0500 | Math word problems |
| HumanEval | 0.0793 | Code generation |
| **ChatCORE** | **0.1298** | **Composite chat quality** |

**Observations**:
- Strong improvement on conversational tasks (ARC-Easy: 0.5174 â†’ 0.4571 apparent drop is actually redistribution toward better calibration)
- Weak math/code performance (expected without domain-specific data)
- Model is **helpful but hallucinatory** â€” perfect for understanding alignment challenges

---

## Key Learnings

### 1. Distributed Training Complexity

Managing 8 GPUs with PyTorch DDP taught me about:
- **Gradient Synchronization**: AllReduce operations across devices
- **Batch Size Scaling**: Total batch = device_batch Ã— num_gpus Ã— gradient_accumulation
- **VRAM Management**: Peak usage 75.4 GiB per GPU (out of 80 GB)
- **Efficient Checkpointing**: Saving optimizer states across multiple devices

### 2. Data Efficiency Matters More Than Raw Compute

The **20:1 token-to-parameter ratio** is a critical heuristic:
- Too few tokens â†’ underfitting
- Too many tokens â†’ diminishing returns (Chinchilla scaling laws suggest 20:1 is near-optimal for smaller models)

**My Takeaway**: For resource-constrained training, carefully curating **high-quality data** (FineWeb-Edu beats raw Common Crawl) gives better ROI than just adding more compute.

### 3. The Fine-Tuning Phase Is Delicate

**Overfitting Risk**: With only 20K chat examples, epoch=1 was crucial. At epoch=2, validation loss increased (could be memorization).

**Instruction Format Matters**: SmolTalk uses a clean `<|user|>...<|assistant|>...` format. Inconsistent formatting breaks chat performance.

### 4. Evaluation Requires Nuance

**CORE/ChatCORE Metrics**: Composite scores across multiple benchmarks give a holistic view, but:
- GSM8K (math) is sensitive to output format (model needs to generate "\n#### 42" style answers)
- HumanEval (code) requires exact syntaxâ€”close doesn't count
- HellaSwag (commonsense) tests world knowledge, not just language

**Human Evaluation**: Metrics don't capture **personality, coherence, or safety**. Playing with the live demo reveals quirks no benchmark shows.

---

## Connections to Biological Modeling

This project directly supports my goal of applying AI to **computational biology**:

**1. Representation Learning**:
- LLMs learn compressed representations of language â†’ similar approaches for gene expression (genes/cells as "tokens", pathways as "context")
- Tokenization strategies â†’ how to discretize continuous biological signals

**2. Self-Supervised Learning**:
- Pretraining on unlabeled text â†’ pretraining on unlabeled spatial transcriptomics (see my [SSL on MOSTA project](/projects/gnn-ssl-mosta))
- Next-token prediction â†’ masking gene expressions and reconstructing

**3. Scale and Efficiency**:
- Training 561M parameters in 8 hours â†’ how to scale graph neural networks on millions of cells?
- Distributed training patterns â†’ processing large tissue atlases

**4. Transfer Learning**:
- LLM fine-tuning â†’ adapting pretrained biological models to new tissues/diseases

---

## Live Demo

ðŸŽ¯ **Try it yourself**: [https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat](https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat)

**Suggested Prompts**:
- "Explain how photosynthesis works"
- "Write a short poem about the cloud"
- "Tell me a story about a robot learning to paint"

**Expected Behavior**: The model will provide coherent, often accurate responses but may hallucinate facts (especially for niche topics) or struggle with multi-step reasoning. But it's good at story telling and teaching with the examples I tried.

---

## Model Files & Reproducibility

All model checkpoints and training reports are available:

**Repository**: [github.com/Thewhey-Brian/nanochat](https://github.com/Thewhey-Brian/nanochat) (forked from Karpathy's original)

**HuggingFace Model**: [BrianGuo/nanochat-20b-chat](https://huggingface.co/BrianGuo/nanochat-d20-chat/tree/main)

**Local Training Artifacts**:
- Custom tokenizer: `tokenizer.pkl`, `token_bytes.pt`
- Base model checkpoint: `base_checkpoints/d20/model_021400.pt`
- Chat SFT checkpoint: `chatsft_checkpoints/d20/model_000650.pt`

---

## Future Directions

**1. Reinforcement Learning (GRPO)**:
- Current model is SFT-only â†’ adding PPO for alignment
- Reward model training on preference data

**2. Efficient Fine-Tuning**:
- LoRA/QLoRA for domain adaptation without full retraining
- Exploring how biological foundation models could use similar techniques

**3. Multilingual Extension**:
- Current tokenizer is English-biased â†’ train on multilingual corpus (maybe in biological language)
- Study how polyglot tokenization affects model capacity

**4. Scaling Laws Exploration**:
- Train d26 (1.9B params) and d32 models to verify Chinchilla scaling predictions
- Quantify compute vs. performance trade-offs

---

## Acknowledgments

**Inspiration**: This work stands on the shoulders of [Andrej Karpathy](https://karpathy.ai/), whose [nanochat project](https://github.com/karpathy/nanochat) makes LLM training accessible to individuals. His educational philosophyâ€”making complex systems hackableâ€”is something I deeply admire.

**Compute**: Lambda Labs GPU Cloud for reliable 8xA100 access

**Data**: HuggingFace for FineWeb-Edu and SmolTalk datasets

**Community**: The open-source ML community for PyTorch, Transformers, and countless tutorials

---
## Sample Model Outputs

Thanks for reading! Here are some little treats - fun outputs from the trained model that showcase its personality and capabilities:

![chinese](/projects/nanochat_llm_training/chinese.png)
*I totally don't know what it's talking about, but it's fun to see the performance.*

![Story](/projects/nanochat_llm_training/story.png)
*Want to know the rest of the story? Try it out yourself!*

---

## Contact

**Questions?** Reach out via [GitHub Issues](https://github.com/Thewhey-Brian/nanochat/issues) or connect with me on LinkedIn.

**Want to collaborate?** I'm actively seeking opportunities in ML research/engineering, particularly at the intersection of deep learning and biology. Let's chat!
