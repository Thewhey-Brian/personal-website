{
  "title": "Building a ChatGPT from Scratch: A Dive into LLM Training",
  "summary": "A hands-on journey through the complete LLM pipelineâ€”tokenization, pretraining, distributed training, and fine-tuningâ€”inspired by Andrej Karpathy's nanochat. Training a 561M parameter conversational model on 11.2B tokens to understand different aspects of modern language models.",
  "status": "completed",
  "role": "ML Engineering & Research",
  "stack": [
    "PyTorch",
    "CUDA",
    "Distributed Training",
    "BPE Tokenization",
    "Transformer Architecture"
  ],
  "images": [],
  "tags": [
    "llm",
    "deep learning",
    "nlp",
    "transformer",
    "distributed training",
    "self-supervised learning",
    "chatgpt"
  ],
  "featured": true,
  "startDate": "2025-10-16T00:00:00.000Z",
  "body": {
    "raw": "\n<div style={{width: \"100%\", height: \"600px\", border: \"1px solid #e5e7eb\", borderRadius: \"8px\", overflow: \"hidden\", marginBottom: \"2rem\"}}>\n  <iframe\n    src=\"https://brianguo-nanochat-20b-chat.hf.space\"\n    width=\"100%\"\n    height=\"100%\"\n    style={{border: \"none\"}}\n    title=\"nanochat-20b-chat Demo\"\n  />\n</div>\n\n*Try the model above â€” it's a 561M parameter model trained from scratch. It makes mistakes, it hallucinates, and it's a bit naive, but it's fun.*\n\n---\n\n## Motivation\n\nLarge Language Models have transformed AI, but their training pipelines remain opaque to myself. When I encountered [Andrej Karpathy's nanochat project](https://github.com/karpathy/nanochat), I saw an opportunity to **get my hands dirty** with every stage of LLM developmentâ€”not just fine-tuning pre-trained models, but building one from the ground up.\n\nThis project wasn't about competing with GPT-4. It was about **understanding the fundamentals**:\n- How does tokenization affect model efficiency?\n- What happens during distributed pretraining?\n- How do you transition from next-token prediction to conversational AI?\n- What trade-offs exist between model size, data, and compute?\n\nAs someone aiming to apply deep learning to **model biological systems**, these foundational skillsâ€”handling large-scale data, distributed training, and representation learningâ€”are directly transferable. The techniques that enable LLMs to compress human language into vector spaces can similarly help compress the complexity of cellular processes, gene expression patterns, and protein interactions.\n\n---\n\n## Technical Overview\n\n### Architecture: GPT-Style Decoder-Only Transformer\n\n**Model Specifications**:\n- **Parameters**: 561 million (d20 configuration: 20 layers)\n- **Context Length**: 2048 tokens\n- **Vocabulary**: 65,536 tokens (custom BPE tokenizer)\n- **Hidden Dimension**: 1024\n- **Attention Heads**: 16\n- **Activation**: GELU\n\n**Training Infrastructure**:\n- **Hardware**: 8Ã— NVIDIA A100-SXM4-80GB GPUs (634GB total VRAM)\n- **Framework**: PyTorch 2.8.0 with Distributed Data Parallel (DDP)\n- **Platform**: Lambda Labs cloud (CUDA 12.8 on Linux)\n- **Total Cost**: ~$120 (8.4 hours at $14.32/hour)\n- **Model FLOPs Utilization (MFU)**: 20.82%\n\n---\n\n## The Complete Training Pipeline\n\n### Stage 1: Custom Tokenizer Training\n\nInstead of using GPT-2's tokenizer, a **custom Byte Pair Encoding (BPE) tokenizer** was trained from scratch on 2 billion characters from the FineWeb-Edu dataset.\n\n**Performance vs. Baselines**:\n\n| Domain | vs GPT-2 | vs GPT-4 |\n|--------|----------|----------|\n| News | **+7.2%** | **+3.1%** |\n| Science | **+12.3%** | **+8.4%** |\n| Code | **+14.4%** | -59.5% |\n| Math | -3.2% | -16.1% |\n\n**Key Insight**: The tokenizer excels at **scientific text and natural language** (the training domain) but underperforms on code and multilingual data (expected, since FineWeb-Edu is English-heavy). This demonstrates the importance of **domain-matched tokenization**, a critical consideration for future applications in computational biology where domain-specific vocabularies (gene names, protein sequences, chemical compounds) require specialized tokenization strategies.\n\n**Compression Ratio**: 4.91 bytes/token (competitive with GPT-2's 4.67 and GPT-4's 4.81)\n\n**Training Time**: 1.6 minutes on 8xA100\n\n---\n\n### Stage 2: Base Model Pretraining (21,400 iterations)\n\nThis is where the model learns **language understanding** through next-token prediction on unlabeled text.\n\n**Training Details**:\n- **Data**: FineWeb-Edu dataset (filtered Wikipedia and educational web content)\n- **Tokens Processed**: 11.2 billion tokens (20:1 token-to-parameter ratio)\n- **Batch Size**: 524,288 tokens (distributed across 8 GPUs)\n- **Learning Rate Schedule**:\n  - Matrix parameters: 0.02 (cosine decay with 20% warmdown)\n  - Embedding: 0.2 (10Ã— higher for faster vocabulary learning)\n  - Unembedding: 0.004 (lower to stabilize output logits)\n- **Optimizer**: AdamW with weight decay 0.0\n- **Training Time**: 6.6 hours\n- **Final Validation Loss**: 0.8156 bits per byte\n\n**Why Different Learning Rates?**\nThis tiered approach (borrowed from nanochat's modded-nanoGPT optimizations) stabilizes training:\n- **High embedding LR**: Vocabulary embeddings need to move quickly to capture semantic relationships\n- **Low unembedding LR**: Output layer overfitting prevention\n- **Medium matrix LR**: Balances generalization and convergence\n\n**Base Model Evaluation (CORE Benchmark)**:\n\n| Task | Score | Interpretation |\n|------|-------|----------------|\n| HellaSwag | 0.2559 | Commonsense reasoning |\n| Winograd | 0.3040 | Pronoun disambiguation |\n| ARC-Easy | 0.5174 | Elementary science questions |\n| ARC-Challenge | 0.1251 | Advanced reasoning |\n| LAMBADA | 0.3775 | Long-range context |\n| SQuAD | 0.2260 | Reading comprehension |\n\n**CORE Score**: 0.2087 (composite metric showing GPT-1.5 level performance)\n\n**Example Completions** (before chat tuning):\n```\nPrompt: \"The capital of France is\"\nOutput: \"Paris. It is the largest city in France and the capital of the country.\"\n\nPrompt: \"The chemical symbol of gold is\"\nOutput: \"Au. It is a soft, silvery-white metal that is malleable and ductile.\"\n```\n\n---\n\n### Stage 3: Midtraining (765 iterations)\n\n**Purpose**: Domain adaptation to chat-style formatting and more diverse text sources.\n\n**Data**: Transition from pure web text to conversational/instruction-following data\n\n**Training Time**: 30 minutes\n\n**Minimum Validation Loss**: 0.3976 bpb (significant drop from base model, indicating successful adaptation)\n\n---\n\n### Stage 4: Supervised Fine-Tuning (SFT) for Chat (651 iterations)\n\nThis stage transforms the base model into a conversational assistant by training on **20,843 human-AI conversations**.\n\n**Training Configuration**:\n- **Data**: SmolTalk dataset (HuggingFace)\n- **Epochs**: 1 (to avoid overfitting on small instruction dataset)\n- **Batch Size**: Effective 32 examples per step (4 per GPU Ã— 8 GPUs)\n- **Loss Masking**: Only compute loss on AI responses (not user prompts)\n- **Training Time**: 24 minutes\n- **Final Validation Loss**: 1.0189\n\n**Chat Model Benchmarks**:\n\n| Benchmark | Score | Task Type |\n|-----------|-------|-----------|\n| ARC-Easy | 0.4571 | Science QA |\n| ARC-Challenge | 0.3430 | Complex reasoning |\n| MMLU | 0.3396 | Multitask knowledge |\n| GSM8K | 0.0500 | Math word problems |\n| HumanEval | 0.0793 | Code generation |\n| **ChatCORE** | **0.1298** | **Composite chat quality** |\n\n**Observations**:\n- Strong improvement on conversational tasks (ARC-Easy: 0.5174 â†’ 0.4571 apparent drop is actually redistribution toward better calibration)\n- Weak math/code performance (expected without domain-specific data)\n- Model is **helpful but hallucinatory** â€” perfect for understanding alignment challenges\n\n---\n\n## Key Learnings\n\n### 1. Distributed Training Complexity\n\nManaging 8 GPUs with PyTorch DDP taught me about:\n- **Gradient Synchronization**: AllReduce operations across devices\n- **Batch Size Scaling**: Total batch = device_batch Ã— num_gpus Ã— gradient_accumulation\n- **VRAM Management**: Peak usage 75.4 GiB per GPU (out of 80 GB)\n- **Efficient Checkpointing**: Saving optimizer states across multiple devices\n\n### 2. Data Efficiency Matters More Than Raw Compute\n\nThe **20:1 token-to-parameter ratio** is a critical heuristic:\n- Too few tokens â†’ underfitting\n- Too many tokens â†’ diminishing returns (Chinchilla scaling laws suggest 20:1 is near-optimal for smaller models)\n\n**My Takeaway**: For resource-constrained training, carefully curating **high-quality data** (FineWeb-Edu beats raw Common Crawl) gives better ROI than just adding more compute.\n\n### 3. The Fine-Tuning Phase Is Delicate\n\n**Overfitting Risk**: With only 20K chat examples, epoch=1 was crucial. At epoch=2, validation loss increased (could be memorization).\n\n**Instruction Format Matters**: SmolTalk uses a clean `<|user|>...<|assistant|>...` format. Inconsistent formatting breaks chat performance.\n\n### 4. Evaluation Requires Nuance\n\n**CORE/ChatCORE Metrics**: Composite scores across multiple benchmarks give a holistic view, but:\n- GSM8K (math) is sensitive to output format (model needs to generate \"\\n#### 42\" style answers)\n- HumanEval (code) requires exact syntaxâ€”close doesn't count\n- HellaSwag (commonsense) tests world knowledge, not just language\n\n**Human Evaluation**: Metrics don't capture **personality, coherence, or safety**. Playing with the live demo reveals quirks no benchmark shows.\n\n---\n\n## Connections to Biological Modeling\n\nThis project directly supports my goal of applying AI to **computational biology**:\n\n**1. Representation Learning**:\n- LLMs learn compressed representations of language â†’ similar approaches for gene expression (genes/cells as \"tokens\", pathways as \"context\")\n- Tokenization strategies â†’ how to discretize continuous biological signals\n\n**2. Self-Supervised Learning**:\n- Pretraining on unlabeled text â†’ pretraining on unlabeled spatial transcriptomics (see my [SSL on MOSTA project](/projects/gnn-ssl-mosta))\n- Next-token prediction â†’ masking gene expressions and reconstructing\n\n**3. Scale and Efficiency**:\n- Training 561M parameters in 8 hours â†’ how to scale graph neural networks on millions of cells?\n- Distributed training patterns â†’ processing large tissue atlases\n\n**4. Transfer Learning**:\n- LLM fine-tuning â†’ adapting pretrained biological models to new tissues/diseases\n\n---\n\n## Live Demo\n\nðŸŽ¯ **Try it yourself**: [https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat](https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat)\n\n**Suggested Prompts**:\n- \"Explain how photosynthesis works\"\n- \"Write a short poem about the cloud\"\n- \"Tell me a story about a robot learning to paint\"\n\n**Expected Behavior**: The model will provide coherent, often accurate responses but may hallucinate facts (especially for niche topics) or struggle with multi-step reasoning. But it's good at story telling and teaching with the examples I tried.\n\n---\n\n## Model Files & Reproducibility\n\nAll model checkpoints and training reports are available:\n\n**Repository**: [github.com/Thewhey-Brian/nanochat](https://github.com/Thewhey-Brian/nanochat) (forked from Karpathy's original)\n\n**HuggingFace Model**: [BrianGuo/nanochat-20b-chat](https://huggingface.co/BrianGuo/nanochat-d20-chat/tree/main)\n\n**Local Training Artifacts**:\n- Custom tokenizer: `tokenizer.pkl`, `token_bytes.pt`\n- Base model checkpoint: `base_checkpoints/d20/model_021400.pt`\n- Chat SFT checkpoint: `chatsft_checkpoints/d20/model_000650.pt`\n\n---\n\n## Future Directions\n\n**1. Reinforcement Learning (GRPO)**:\n- Current model is SFT-only â†’ adding PPO for alignment\n- Reward model training on preference data\n\n**2. Efficient Fine-Tuning**:\n- LoRA/QLoRA for domain adaptation without full retraining\n- Exploring how biological foundation models could use similar techniques\n\n**3. Multilingual Extension**:\n- Current tokenizer is English-biased â†’ train on multilingual corpus (maybe in biological language)\n- Study how polyglot tokenization affects model capacity\n\n**4. Scaling Laws Exploration**:\n- Train d26 (1.9B params) and d32 models to verify Chinchilla scaling predictions\n- Quantify compute vs. performance trade-offs\n\n---\n\n## Acknowledgments\n\n**Inspiration**: This work stands on the shoulders of [Andrej Karpathy](https://karpathy.ai/), whose [nanochat project](https://github.com/karpathy/nanochat) makes LLM training accessible to individuals. His educational philosophyâ€”making complex systems hackableâ€”is something I deeply admire.\n\n**Compute**: Lambda Labs GPU Cloud for reliable 8xA100 access\n\n**Data**: HuggingFace for FineWeb-Edu and SmolTalk datasets\n\n**Community**: The open-source ML community for PyTorch, Transformers, and countless tutorials\n\n---\n## Sample Model Outputs\n\nThanks for reading! Here are some little treats - fun outputs from the trained model that showcase its personality and capabilities:\n\n![chinese](/projects/nanochat_llm_training/chinese.png)\n*I totally don't know what it's talking about, but it's fun to see the performance.*\n\n![Story](/projects/nanochat_llm_training/story.png)\n*Want to know the rest of the story? Try it out yourself!*\n\n---\n\n## Contact\n\n**Questions?** Reach out via [GitHub Issues](https://github.com/Thewhey-Brian/nanochat/issues) or connect with me on LinkedIn.\n\n**Want to collaborate?** I'm actively seeking opportunities in ML research/engineering, particularly at the intersection of deep learning and biology. Let's chat!\n",
    "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(i,n)=>()=>(n||i((n={exports:{}}).exports,n),n.exports),y=(i,n)=>{for(var r in n)a(i,r,{get:n[r],enumerable:!0})},c=(i,n,r,l)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let t of p(n))!f.call(i,t)&&t!==r&&a(i,t,{get:()=>n[t],enumerable:!(l=g(n,t))||l.enumerable});return i};var k=(i,n,r)=>(r=i!=null?m(u(i)):{},c(n||!i||!i.__esModule?a(r,\"default\",{value:i,enumerable:!0}):r,i)),w=i=>c(a({},\"__esModule\",{value:!0}),i);var s=b((M,o)=>{o.exports=_jsx_runtime});var v={};y(v,{default:()=>h,frontmatter:()=>T});var e=k(s()),T={title:\"Building a ChatGPT from Scratch: A Dive into LLM Training\",summary:\"A hands-on journey through the complete LLM pipeline\\u2014tokenization, pretraining, distributed training, and fine-tuning\\u2014inspired by Andrej Karpathy's nanochat. Training a 561M parameter conversational model on 11.2B tokens to understand different aspects of modern language models.\",status:\"completed\",role:\"ML Engineering & Research\",stack:[\"PyTorch\",\"CUDA\",\"Distributed Training\",\"BPE Tokenization\",\"Transformer Architecture\"],tags:[\"llm\",\"deep learning\",\"nlp\",\"transformer\",\"distributed training\",\"self-supervised learning\",\"chatgpt\"],featured:!0,startDate:\"2025-10-16\"};function d(i){let n={a:\"a\",annotation:\"annotation\",code:\"code\",em:\"em\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",math:\"math\",mi:\"mi\",mn:\"mn\",mo:\"mo\",mrow:\"mrow\",p:\"p\",pre:\"pre\",semantics:\"semantics\",span:\"span\",strong:\"strong\",table:\"table\",tbody:\"tbody\",td:\"td\",th:\"th\",thead:\"thead\",tr:\"tr\",ul:\"ul\",...i.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(\"div\",{style:{width:\"100%\",height:\"600px\",border:\"1px solid #e5e7eb\",borderRadius:\"8px\",overflow:\"hidden\",marginBottom:\"2rem\"},children:(0,e.jsx)(\"iframe\",{src:\"https://brianguo-nanochat-20b-chat.hf.space\",width:\"100%\",height:\"100%\",style:{border:\"none\"},title:\"nanochat-20b-chat Demo\"})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.em,{children:\"Try the model above \\u2014 it's a 561M parameter model trained from scratch. It makes mistakes, it hallucinates, and it's a bit naive, but it's fun.\"})}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"motivation\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#motivation\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Motivation\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"Large Language Models have transformed AI, but their training pipelines remain opaque to myself. When I encountered \",(0,e.jsx)(n.a,{href:\"https://github.com/karpathy/nanochat\",children:\"Andrej Karpathy's nanochat project\"}),\", I saw an opportunity to \",(0,e.jsx)(n.strong,{children:\"get my hands dirty\"}),\" with every stage of LLM development\\u2014not just fine-tuning pre-trained models, but building one from the ground up.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"This project wasn't about competing with GPT-4. It was about \",(0,e.jsx)(n.strong,{children:\"understanding the fundamentals\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"How does tokenization affect model efficiency?\"}),`\n`,(0,e.jsx)(n.li,{children:\"What happens during distributed pretraining?\"}),`\n`,(0,e.jsx)(n.li,{children:\"How do you transition from next-token prediction to conversational AI?\"}),`\n`,(0,e.jsx)(n.li,{children:\"What trade-offs exist between model size, data, and compute?\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[\"As someone aiming to apply deep learning to \",(0,e.jsx)(n.strong,{children:\"model biological systems\"}),\", these foundational skills\\u2014handling large-scale data, distributed training, and representation learning\\u2014are directly transferable. The techniques that enable LLMs to compress human language into vector spaces can similarly help compress the complexity of cellular processes, gene expression patterns, and protein interactions.\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"technical-overview\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#technical-overview\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Technical Overview\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"architecture-gpt-style-decoder-only-transformer\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#architecture-gpt-style-decoder-only-transformer\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Architecture: GPT-Style Decoder-Only Transformer\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Model Specifications\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Parameters\"}),\": 561 million (d20 configuration: 20 layers)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Context Length\"}),\": 2048 tokens\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Vocabulary\"}),\": 65,536 tokens (custom BPE tokenizer)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Hidden Dimension\"}),\": 1024\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Attention Heads\"}),\": 16\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Activation\"}),\": GELU\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Training Infrastructure\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Hardware\"}),\": 8\\xD7 NVIDIA A100-SXM4-80GB GPUs (634GB total VRAM)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Framework\"}),\": PyTorch 2.8.0 with Distributed Data Parallel (DDP)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Platform\"}),\": Lambda Labs cloud (CUDA 12.8 on Linux)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Total Cost\"}),\": ~\",(0,e.jsxs)(n.span,{className:\"katex\",children:[(0,e.jsx)(n.span,{className:\"katex-mathml\",children:(0,e.jsx)(n.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:(0,e.jsxs)(n.semantics,{children:[(0,e.jsxs)(n.mrow,{children:[(0,e.jsx)(n.mn,{children:\"120\"}),(0,e.jsx)(n.mo,{stretchy:\"false\",children:\"(\"}),(0,e.jsx)(n.mn,{children:\"8.4\"}),(0,e.jsx)(n.mi,{children:\"h\"}),(0,e.jsx)(n.mi,{children:\"o\"}),(0,e.jsx)(n.mi,{children:\"u\"}),(0,e.jsx)(n.mi,{children:\"r\"}),(0,e.jsx)(n.mi,{children:\"s\"}),(0,e.jsx)(n.mi,{children:\"a\"}),(0,e.jsx)(n.mi,{children:\"t\"})]}),(0,e.jsx)(n.annotation,{encoding:\"application/x-tex\",children:\"120 (8.4 hours at \"})]})})}),(0,e.jsx)(n.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:(0,e.jsxs)(n.span,{className:\"base\",children:[(0,e.jsx)(n.span,{className:\"strut\",style:{height:\"1em\",verticalAlign:\"-0.25em\"}}),(0,e.jsx)(n.span,{className:\"mord\",children:\"120\"}),(0,e.jsx)(n.span,{className:\"mopen\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"mord\",children:\"8.4\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"h\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"o\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"u\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"rs\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"a\"}),(0,e.jsx)(n.span,{className:\"mord mathnormal\",children:\"t\"})]})})]}),\"14.32/hour)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Model FLOPs Utilization (MFU)\"}),\": 20.82%\"]}),`\n`]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"the-complete-training-pipeline\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#the-complete-training-pipeline\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"The Complete Training Pipeline\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"stage-1-custom-tokenizer-training\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#stage-1-custom-tokenizer-training\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Stage 1: Custom Tokenizer Training\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"Instead of using GPT-2's tokenizer, a \",(0,e.jsx)(n.strong,{children:\"custom Byte Pair Encoding (BPE) tokenizer\"}),\" was trained from scratch on 2 billion characters from the FineWeb-Edu dataset.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Performance vs. Baselines\"}),\":\"]}),`\n`,(0,e.jsxs)(n.table,{children:[(0,e.jsx)(n.thead,{children:(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.th,{children:\"Domain\"}),(0,e.jsx)(n.th,{children:\"vs GPT-2\"}),(0,e.jsx)(n.th,{children:\"vs GPT-4\"})]})}),(0,e.jsxs)(n.tbody,{children:[(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"News\"}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"+7.2%\"})}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"+3.1%\"})})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"Science\"}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"+12.3%\"})}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"+8.4%\"})})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"Code\"}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"+14.4%\"})}),(0,e.jsx)(n.td,{children:\"-59.5%\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"Math\"}),(0,e.jsx)(n.td,{children:\"-3.2%\"}),(0,e.jsx)(n.td,{children:\"-16.1%\"})]})]})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Key Insight\"}),\": The tokenizer excels at \",(0,e.jsx)(n.strong,{children:\"scientific text and natural language\"}),\" (the training domain) but underperforms on code and multilingual data (expected, since FineWeb-Edu is English-heavy). This demonstrates the importance of \",(0,e.jsx)(n.strong,{children:\"domain-matched tokenization\"}),\", a critical consideration for future applications in computational biology where domain-specific vocabularies (gene names, protein sequences, chemical compounds) require specialized tokenization strategies.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Compression Ratio\"}),\": 4.91 bytes/token (competitive with GPT-2's 4.67 and GPT-4's 4.81)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Training Time\"}),\": 1.6 minutes on 8xA100\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h3,{id:\"stage-2-base-model-pretraining-21400-iterations\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#stage-2-base-model-pretraining-21400-iterations\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Stage 2: Base Model Pretraining (21,400 iterations)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"This is where the model learns \",(0,e.jsx)(n.strong,{children:\"language understanding\"}),\" through next-token prediction on unlabeled text.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Training Details\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Data\"}),\": FineWeb-Edu dataset (filtered Wikipedia and educational web content)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Tokens Processed\"}),\": 11.2 billion tokens (20:1 token-to-parameter ratio)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Batch Size\"}),\": 524,288 tokens (distributed across 8 GPUs)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Learning Rate Schedule\"}),\":\",`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Matrix parameters: 0.02 (cosine decay with 20% warmdown)\"}),`\n`,(0,e.jsx)(n.li,{children:\"Embedding: 0.2 (10\\xD7 higher for faster vocabulary learning)\"}),`\n`,(0,e.jsx)(n.li,{children:\"Unembedding: 0.004 (lower to stabilize output logits)\"}),`\n`]}),`\n`]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Optimizer\"}),\": AdamW with weight decay 0.0\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Training Time\"}),\": 6.6 hours\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Final Validation Loss\"}),\": 0.8156 bits per byte\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Why Different Learning Rates?\"}),`\nThis tiered approach (borrowed from nanochat's modded-nanoGPT optimizations) stabilizes training:`]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"High embedding LR\"}),\": Vocabulary embeddings need to move quickly to capture semantic relationships\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Low unembedding LR\"}),\": Output layer overfitting prevention\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Medium matrix LR\"}),\": Balances generalization and convergence\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Base Model Evaluation (CORE Benchmark)\"}),\":\"]}),`\n`,(0,e.jsxs)(n.table,{children:[(0,e.jsx)(n.thead,{children:(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.th,{children:\"Task\"}),(0,e.jsx)(n.th,{children:\"Score\"}),(0,e.jsx)(n.th,{children:\"Interpretation\"})]})}),(0,e.jsxs)(n.tbody,{children:[(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"HellaSwag\"}),(0,e.jsx)(n.td,{children:\"0.2559\"}),(0,e.jsx)(n.td,{children:\"Commonsense reasoning\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"Winograd\"}),(0,e.jsx)(n.td,{children:\"0.3040\"}),(0,e.jsx)(n.td,{children:\"Pronoun disambiguation\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"ARC-Easy\"}),(0,e.jsx)(n.td,{children:\"0.5174\"}),(0,e.jsx)(n.td,{children:\"Elementary science questions\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"ARC-Challenge\"}),(0,e.jsx)(n.td,{children:\"0.1251\"}),(0,e.jsx)(n.td,{children:\"Advanced reasoning\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"LAMBADA\"}),(0,e.jsx)(n.td,{children:\"0.3775\"}),(0,e.jsx)(n.td,{children:\"Long-range context\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"SQuAD\"}),(0,e.jsx)(n.td,{children:\"0.2260\"}),(0,e.jsx)(n.td,{children:\"Reading comprehension\"})]})]})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"CORE Score\"}),\": 0.2087 (composite metric showing GPT-1.5 level performance)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Example Completions\"}),\" (before chat tuning):\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{children:`Prompt: \"The capital of France is\"\nOutput: \"Paris. It is the largest city in France and the capital of the country.\"\n\nPrompt: \"The chemical symbol of gold is\"\nOutput: \"Au. It is a soft, silvery-white metal that is malleable and ductile.\"\n`})}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h3,{id:\"stage-3-midtraining-765-iterations\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#stage-3-midtraining-765-iterations\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Stage 3: Midtraining (765 iterations)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Purpose\"}),\": Domain adaptation to chat-style formatting and more diverse text sources.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Data\"}),\": Transition from pure web text to conversational/instruction-following data\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Training Time\"}),\": 30 minutes\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Minimum Validation Loss\"}),\": 0.3976 bpb (significant drop from base model, indicating successful adaptation)\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h3,{id:\"stage-4-supervised-fine-tuning-sft-for-chat-651-iterations\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#stage-4-supervised-fine-tuning-sft-for-chat-651-iterations\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Stage 4: Supervised Fine-Tuning (SFT) for Chat (651 iterations)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"This stage transforms the base model into a conversational assistant by training on \",(0,e.jsx)(n.strong,{children:\"20,843 human-AI conversations\"}),\".\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Training Configuration\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Data\"}),\": SmolTalk dataset (HuggingFace)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Epochs\"}),\": 1 (to avoid overfitting on small instruction dataset)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Batch Size\"}),\": Effective 32 examples per step (4 per GPU \\xD7 8 GPUs)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Loss Masking\"}),\": Only compute loss on AI responses (not user prompts)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Training Time\"}),\": 24 minutes\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Final Validation Loss\"}),\": 1.0189\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Chat Model Benchmarks\"}),\":\"]}),`\n`,(0,e.jsxs)(n.table,{children:[(0,e.jsx)(n.thead,{children:(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.th,{children:\"Benchmark\"}),(0,e.jsx)(n.th,{children:\"Score\"}),(0,e.jsx)(n.th,{children:\"Task Type\"})]})}),(0,e.jsxs)(n.tbody,{children:[(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"ARC-Easy\"}),(0,e.jsx)(n.td,{children:\"0.4571\"}),(0,e.jsx)(n.td,{children:\"Science QA\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"ARC-Challenge\"}),(0,e.jsx)(n.td,{children:\"0.3430\"}),(0,e.jsx)(n.td,{children:\"Complex reasoning\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"MMLU\"}),(0,e.jsx)(n.td,{children:\"0.3396\"}),(0,e.jsx)(n.td,{children:\"Multitask knowledge\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"GSM8K\"}),(0,e.jsx)(n.td,{children:\"0.0500\"}),(0,e.jsx)(n.td,{children:\"Math word problems\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:\"HumanEval\"}),(0,e.jsx)(n.td,{children:\"0.0793\"}),(0,e.jsx)(n.td,{children:\"Code generation\"})]}),(0,e.jsxs)(n.tr,{children:[(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"ChatCORE\"})}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"0.1298\"})}),(0,e.jsx)(n.td,{children:(0,e.jsx)(n.strong,{children:\"Composite chat quality\"})})]})]})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Observations\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Strong improvement on conversational tasks (ARC-Easy: 0.5174 \\u2192 0.4571 apparent drop is actually redistribution toward better calibration)\"}),`\n`,(0,e.jsx)(n.li,{children:\"Weak math/code performance (expected without domain-specific data)\"}),`\n`,(0,e.jsxs)(n.li,{children:[\"Model is \",(0,e.jsx)(n.strong,{children:\"helpful but hallucinatory\"}),\" \\u2014 perfect for understanding alignment challenges\"]}),`\n`]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"key-learnings\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#key-learnings\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Key Learnings\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"1-distributed-training-complexity\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#1-distributed-training-complexity\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"1. Distributed Training Complexity\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Managing 8 GPUs with PyTorch DDP taught me about:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Gradient Synchronization\"}),\": AllReduce operations across devices\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Batch Size Scaling\"}),\": Total batch = device_batch \\xD7 num_gpus \\xD7 gradient_accumulation\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"VRAM Management\"}),\": Peak usage 75.4 GiB per GPU (out of 80 GB)\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Efficient Checkpointing\"}),\": Saving optimizer states across multiple devices\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.h3,{id:\"2-data-efficiency-matters-more-than-raw-compute\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#2-data-efficiency-matters-more-than-raw-compute\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"2. Data Efficiency Matters More Than Raw Compute\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"The \",(0,e.jsx)(n.strong,{children:\"20:1 token-to-parameter ratio\"}),\" is a critical heuristic:\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Too few tokens \\u2192 underfitting\"}),`\n`,(0,e.jsx)(n.li,{children:\"Too many tokens \\u2192 diminishing returns (Chinchilla scaling laws suggest 20:1 is near-optimal for smaller models)\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"My Takeaway\"}),\": For resource-constrained training, carefully curating \",(0,e.jsx)(n.strong,{children:\"high-quality data\"}),\" (FineWeb-Edu beats raw Common Crawl) gives better ROI than just adding more compute.\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"3-the-fine-tuning-phase-is-delicate\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#3-the-fine-tuning-phase-is-delicate\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"3. The Fine-Tuning Phase Is Delicate\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Overfitting Risk\"}),\": With only 20K chat examples, epoch=1 was crucial. At epoch=2, validation loss increased (could be memorization).\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Instruction Format Matters\"}),\": SmolTalk uses a clean \",(0,e.jsx)(n.code,{children:\"<|user|>...<|assistant|>...\"}),\" format. Inconsistent formatting breaks chat performance.\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"4-evaluation-requires-nuance\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#4-evaluation-requires-nuance\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"4. Evaluation Requires Nuance\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"CORE/ChatCORE Metrics\"}),\": Composite scores across multiple benchmarks give a holistic view, but:\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:'GSM8K (math) is sensitive to output format (model needs to generate \"\\\\n#### 42\" style answers)'}),`\n`,(0,e.jsx)(n.li,{children:\"HumanEval (code) requires exact syntax\\u2014close doesn't count\"}),`\n`,(0,e.jsx)(n.li,{children:\"HellaSwag (commonsense) tests world knowledge, not just language\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Human Evaluation\"}),\": Metrics don't capture \",(0,e.jsx)(n.strong,{children:\"personality, coherence, or safety\"}),\". Playing with the live demo reveals quirks no benchmark shows.\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"connections-to-biological-modeling\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#connections-to-biological-modeling\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Connections to Biological Modeling\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"This project directly supports my goal of applying AI to \",(0,e.jsx)(n.strong,{children:\"computational biology\"}),\":\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"1. Representation Learning\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:'LLMs learn compressed representations of language \\u2192 similar approaches for gene expression (genes/cells as \"tokens\", pathways as \"context\")'}),`\n`,(0,e.jsx)(n.li,{children:\"Tokenization strategies \\u2192 how to discretize continuous biological signals\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"2. Self-Supervised Learning\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[\"Pretraining on unlabeled text \\u2192 pretraining on unlabeled spatial transcriptomics (see my \",(0,e.jsx)(n.a,{href:\"/projects/gnn-ssl-mosta\",children:\"SSL on MOSTA project\"}),\")\"]}),`\n`,(0,e.jsx)(n.li,{children:\"Next-token prediction \\u2192 masking gene expressions and reconstructing\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"3. Scale and Efficiency\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Training 561M parameters in 8 hours \\u2192 how to scale graph neural networks on millions of cells?\"}),`\n`,(0,e.jsx)(n.li,{children:\"Distributed training patterns \\u2192 processing large tissue atlases\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"4. Transfer Learning\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"LLM fine-tuning \\u2192 adapting pretrained biological models to new tissues/diseases\"}),`\n`]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"live-demo\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#live-demo\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Live Demo\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"\\u{1F3AF} \",(0,e.jsx)(n.strong,{children:\"Try it yourself\"}),\": \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat\",children:\"https://huggingface.co/spaces/BrianGuo/nanochat-20b-chat\"})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Suggested Prompts\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:'\"Explain how photosynthesis works\"'}),`\n`,(0,e.jsx)(n.li,{children:'\"Write a short poem about the cloud\"'}),`\n`,(0,e.jsx)(n.li,{children:'\"Tell me a story about a robot learning to paint\"'}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Expected Behavior\"}),\": The model will provide coherent, often accurate responses but may hallucinate facts (especially for niche topics) or struggle with multi-step reasoning. But it's good at story telling and teaching with the examples I tried.\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"model-files--reproducibility\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#model-files--reproducibility\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Model Files & Reproducibility\"]}),`\n`,(0,e.jsx)(n.p,{children:\"All model checkpoints and training reports are available:\"}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Repository\"}),\": \",(0,e.jsx)(n.a,{href:\"https://github.com/Thewhey-Brian/nanochat\",children:\"github.com/Thewhey-Brian/nanochat\"}),\" (forked from Karpathy's original)\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"HuggingFace Model\"}),\": \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/BrianGuo/nanochat-d20-chat/tree/main\",children:\"BrianGuo/nanochat-20b-chat\"})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Local Training Artifacts\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[\"Custom tokenizer: \",(0,e.jsx)(n.code,{children:\"tokenizer.pkl\"}),\", \",(0,e.jsx)(n.code,{children:\"token_bytes.pt\"})]}),`\n`,(0,e.jsxs)(n.li,{children:[\"Base model checkpoint: \",(0,e.jsx)(n.code,{children:\"base_checkpoints/d20/model_021400.pt\"})]}),`\n`,(0,e.jsxs)(n.li,{children:[\"Chat SFT checkpoint: \",(0,e.jsx)(n.code,{children:\"chatsft_checkpoints/d20/model_000650.pt\"})]}),`\n`]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"future-directions\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#future-directions\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Future Directions\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"1. Reinforcement Learning (GRPO)\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Current model is SFT-only \\u2192 adding PPO for alignment\"}),`\n`,(0,e.jsx)(n.li,{children:\"Reward model training on preference data\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"2. Efficient Fine-Tuning\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"LoRA/QLoRA for domain adaptation without full retraining\"}),`\n`,(0,e.jsx)(n.li,{children:\"Exploring how biological foundation models could use similar techniques\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"3. Multilingual Extension\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Current tokenizer is English-biased \\u2192 train on multilingual corpus (maybe in biological language)\"}),`\n`,(0,e.jsx)(n.li,{children:\"Study how polyglot tokenization affects model capacity\"}),`\n`]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"4. Scaling Laws Exploration\"}),\":\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Train d26 (1.9B params) and d32 models to verify Chinchilla scaling predictions\"}),`\n`,(0,e.jsx)(n.li,{children:\"Quantify compute vs. performance trade-offs\"}),`\n`]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"acknowledgments\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#acknowledgments\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Acknowledgments\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Inspiration\"}),\": This work stands on the shoulders of \",(0,e.jsx)(n.a,{href:\"https://karpathy.ai/\",children:\"Andrej Karpathy\"}),\", whose \",(0,e.jsx)(n.a,{href:\"https://github.com/karpathy/nanochat\",children:\"nanochat project\"}),\" makes LLM training accessible to individuals. His educational philosophy\\u2014making complex systems hackable\\u2014is something I deeply admire.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Compute\"}),\": Lambda Labs GPU Cloud for reliable 8xA100 access\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Data\"}),\": HuggingFace for FineWeb-Edu and SmolTalk datasets\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Community\"}),\": The open-source ML community for PyTorch, Transformers, and countless tutorials\"]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"sample-model-outputs\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#sample-model-outputs\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Sample Model Outputs\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Thanks for reading! Here are some little treats - fun outputs from the trained model that showcase its personality and capabilities:\"}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.img,{src:\"/projects/nanochat_llm_training/chinese.png\",alt:\"chinese\"}),`\n`,(0,e.jsx)(n.em,{children:\"I totally don't know what it's talking about, but it's fun to see the performance.\"})]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.img,{src:\"/projects/nanochat_llm_training/story.png\",alt:\"Story\"}),`\n`,(0,e.jsx)(n.em,{children:\"Want to know the rest of the story? Try it out yourself!\"})]}),`\n`,(0,e.jsx)(n.hr,{}),`\n`,(0,e.jsxs)(n.h2,{id:\"contact\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#contact\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Contact\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Questions?\"}),\" Reach out via \",(0,e.jsx)(n.a,{href:\"https://github.com/Thewhey-Brian/nanochat/issues\",children:\"GitHub Issues\"}),\" or connect with me on LinkedIn.\"]}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Want to collaborate?\"}),\" I'm actively seeking opportunities in ML research/engineering, particularly at the intersection of deep learning and biology. Let's chat!\"]})]})}function h(i={}){let{wrapper:n}=i.components||{};return n?(0,e.jsx)(n,{...i,children:(0,e.jsx)(d,{...i})}):d(i)}return w(v);})();\n;return Component;"
  },
  "_id": "projects/nanochat-llm-training.mdx",
  "_raw": {
    "sourceFilePath": "projects/nanochat-llm-training.mdx",
    "sourceFileName": "nanochat-llm-training.mdx",
    "sourceFileDir": "projects",
    "contentType": "mdx",
    "flattenedPath": "projects/nanochat-llm-training"
  },
  "type": "Project",
  "url": "/projects/nanochat-llm-training",
  "slug": "nanochat-llm-training"
}